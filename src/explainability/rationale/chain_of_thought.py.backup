import logging
from typing import Dict, List, Optional, Any, Tuple
from PIL import Image
import json
import numpy as np

from .medical_knowledge_base import MedicalKnowledgeBase
from .evidence_linker import EvidenceLinker
from .reasoning_templates import ReasoningTemplates

logger = logging.getLogger(__name__)

class ChainOfThoughtGenerator:
    """
    Chain-of-Thought Generator for MedXplain-VQA
    
    Generates structured medical reasoning chains that link visual evidence
    to diagnostic conclusions through step-by-step reasoning process.
    """
    
    def __init__(self, gemini_integration, config):
        """
        Initialize Chain-of-Thought Generator
        
        Args:
            gemini_integration: GeminiIntegration instance
            config: Configuration object
        """
        self.gemini = gemini_integration
        self.config = config
        
        # Initialize components
        self.knowledge_base = MedicalKnowledgeBase(config)
        self.evidence_linker = EvidenceLinker(config)
        self.templates = ReasoningTemplates()
        
        # Reasoning configuration
        self.reasoning_config = {
            'default_flow': config.get('explainability.reasoning.default_flow', 'standard_diagnostic'),
            'confidence_threshold': config.get('explainability.reasoning.confidence_threshold', 0.5),
            'max_reasoning_steps': config.get('explainability.reasoning.max_steps', 8),
            'enable_differential': config.get('explainability.reasoning.enable_differential', True)
        }
        
        # IMPROVED: Confidence calculation parameters
        self.confidence_params = {
            'base_confidence': 0.75,  # Increased from 0.7
            'evidence_weight': 0.3,   # Weight of evidence contribution
            'step_reliability': {     # Step-specific reliability scores
                'visual_observation': 0.90,
                'attention_analysis': 0.85,
                'feature_extraction': 0.82,
                'clinical_correlation': 0.78,
                'pathological_assessment': 0.75,
                'differential_diagnosis': 0.72,
                'diagnostic_reasoning': 0.80,
                'conclusion': 0.85
            },
            'evidence_multipliers': {  # Improved evidence scoring
                'high': 1.0,
                'moderate': 0.95,
                'low': 0.90
            },
            'chain_confidence_method': 'weighted_harmonic_mean'  # Better than multiplicative
        }
        
        logger.info("Chain-of-Thought Generator initialized with improved confidence calculation")
    
    def generate_reasoning_chain(self, image: Image.Image, 
                               reformulated_question: str,
                               blip_answer: str,
                               visual_context: Dict,
                               grad_cam_data: Optional[Dict] = None) -> Dict:
        """
        Generate complete chain-of-thought reasoning
        
        Args:
            image: PIL Image
            reformulated_question: Reformulated question from Phase 3A
            blip_answer: Initial BLIP answer
            visual_context: Visual context from VisualContextExtractor
            grad_cam_data: Grad-CAM attention data (optional)
            
        Returns:
            Complete reasoning chain dictionary
        """
        logger.info("Generating chain-of-thought reasoning")
        
        try:
            # Step 1: Extract and link visual evidence
            visual_evidence = self._extract_visual_evidence(image, grad_cam_data, visual_context)
            
            # Step 2: Identify anatomical and pathological context
            medical_context = self._identify_medical_context(visual_context, visual_evidence)
            
            # Step 3: Determine reasoning flow
            reasoning_flow = self._select_reasoning_flow(reformulated_question, medical_context)
            
            # Step 4: Generate reasoning steps
            reasoning_steps = self._generate_reasoning_steps(
                image, reformulated_question, blip_answer, 
                visual_context, visual_evidence, medical_context, reasoning_flow
            )
            
            # Step 5: Link evidence to steps
            enhanced_steps = self._link_evidence_to_steps(reasoning_steps, visual_evidence)
            
            # Step 6: Create complete reasoning chain
            reasoning_chain = self._create_reasoning_chain(
                enhanced_steps, reasoning_flow, visual_evidence, medical_context
            )
            
            # Step 7: Validate reasoning
    def _validate_reasoning_chain(self, reasoning_chain: Dict) -> Dict:
        """Validate generated reasoning chain with improved scoring"""
        logger.debug("Validating reasoning chain")
        
        chain_data = reasoning_chain.get("reasoning_chain", {})
        
        # Use templates validation
        template_validation = self.templates.validate_reasoning_chain(chain_data)
        
        # Add medical knowledge validation
        steps = chain_data.get("steps", [])
        medical_validation = self.knowledge_base.validate_clinical_reasoning(steps)
        
        # IMPROVED: Confidence-aware validation
        overall_confidence = chain_data.get("overall_confidence", 0.0)
        confidence_validity = overall_confidence >= 0.5
        
        # FIXED: Robust combined score calculation
        template_completeness = template_validation.get("completeness_score", 0.0) or 0.0
        template_consistency = template_validation.get("consistency_score", 0.0) or 0.0
        medical_accuracy = medical_validation.get("medical_accuracy_score", 0.0) or 0.0
        medical_consistency = medical_validation.get("logical_consistency_score", 0.0) or 0.0
        
        # Calculate combined score with fallback
        scores = [template_completeness, template_consistency, medical_accuracy, medical_consistency, overall_confidence]
        valid_scores = [s for s in scores if isinstance(s, (int, float)) and s > 0]
        
        if valid_scores:
            combined_score = sum(valid_scores) / len(valid_scores)
        else:
            combined_score = overall_confidence  # Fallback to confidence
        
        # Combine validations
        combined_validation = {
            "template_validation": template_validation,
            "medical_validation": medical_validation,
            "confidence_validation": {
                "confidence_level": overall_confidence,
                "meets_threshold": confidence_validity,
                "confidence_category": self._categorize_confidence(overall_confidence)
            },
            "overall_validity": (template_validation["is_valid"] and 
                               medical_validation["overall_validity"] and 
                               confidence_validity),
            "combined_score": combined_score
        }
        
        return combined_validation
    
    def _categorize_confidence(self, confidence: float) -> str:
        """Categorize confidence level"""
        if confidence >= 0.8:
            return 'high'
        elif confidence >= 0.65:
            return 'moderate-high'
        elif confidence >= 0.5:
            return 'moderate'
        elif confidence >= 0.35:
            return 'low-moderate'
        else:
            return 'low'
    
    def _get_timestamp(self) -> str:
        """Get current timestamp"""
        from datetime import datetime
        return datetime.now().isoformat()
    
    def save_reasoning_chain(self, reasoning_chain: Dict, output_path: str):
        """
        Save reasoning chain to file
        
        Args:
            reasoning_chain: Complete reasoning chain
            output_path: Output file path
        """
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(reasoning_chain, f, indent=2, ensure_ascii=False)
            
            logger.info(f"Reasoning chain saved to {output_path}")
            
        except Exception as e:
            logger.error(f"Error saving reasoning chain: {e}")
